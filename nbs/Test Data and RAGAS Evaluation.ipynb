{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58299dcb-5b31-4bf8-a35e-29c6ec4fbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tiktoken\n",
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "from langchain_core.prompts import (ChatMessagePromptTemplate, SystemMessagePromptTemplate, \n",
    "                                    AIMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "# RAGAS imports\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebc849d2-a823-4378-880b-71df8ae5ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv; _ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5ca65-faa1-48a0-ba8b-3f5c7a45124d",
   "metadata": {},
   "source": [
    "# Synthetic Test Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e7efb-97b3-4735-80d3-3bd92ce1fe1f",
   "metadata": {},
   "source": [
    "## Index relevant documents\n",
    "We begin by first loading and indexing documents related to AI Ethics. These documents will be used to generate synthetic test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e6508f3-2391-4e9f-8401-89489f57025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "pdf_loader = PyMuPDFLoader('Blueprint-for-an-AI-Bill-of-Rights.pdf')\n",
    "docs_bill_of_rights = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae9ad4d-887f-4eb3-b085-12dd3afa8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyMuPDFLoader('NIST.AI.600-1.pdf')\n",
    "docs_nist = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5be4226-0cf4-458f-822a-e2c77c03ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = docs_bill_of_rights + docs_nist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc63a57d-faa6-4d9f-8759-43a9d6ef9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents in preparation for SDG\n",
    "def tiktoken_len(text, model='text-embedding-3-small'):\n",
    "    embedding = tiktoken.encoding_for_model(model)\n",
    "    query = embedding.encode(text)\n",
    "    return len(query)\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    length_function=tiktoken_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f77c6c6-abaf-4c76-bd15-7fb11a03f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f00bdb4c-18ae-4977-b189-a2cc6815a132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2f5197eba494411283efc7efce5a8960',\n",
       " '2012da8723dd46808ac7f96e922aefb1',\n",
       " '9a68d8eb03bf4a27a6a4562aca5e791e',\n",
       " 'ac265d97ebd44634b2f26a779a1bfb28',\n",
       " 'c01bdb229a7041978cc124a5e1332ba7',\n",
       " '59d6abb72460429fbb198d274dc55c58',\n",
       " '3b09e720beb84d72b1e3c2a3cc728750',\n",
       " 'e3cde20aa5764e7d91e5548101505bf7',\n",
       " 'ea6c8e687ec448b394247e485df0cbf9',\n",
       " '1b9d9265d6f2433ab3ee4cfe1c2d5054',\n",
       " '3f82bb4754f440bfa83369a3d169aaed',\n",
       " '727f0defb1744bb6802db51177099bf5',\n",
       " '715ab65b957e4c3d9a3eeed4cac362fe',\n",
       " '77f86b63c75b4693bdebbeeb459bed66',\n",
       " 'bb20387ec91e47e78610bcbaf08d1151',\n",
       " '8d486d77d52e4ed78f7fdf356f6a87f4',\n",
       " '523e5f60aeea408fb28e1501145f1067',\n",
       " '885f99f398da4007b348567d33b99fc9',\n",
       " '9e4d5a52c6f64356afea9e81cd1d31dd',\n",
       " '51183dc3ba374fb39eee25597ced6a91',\n",
       " '645c08de20ef40edbbf4c32a4b80a7b7',\n",
       " 'd540a540aa3c4ba892b12047f4bef6a7',\n",
       " 'f057a3b9021e403597f532472388bc0b',\n",
       " 'edfd39342f554800a0dc7d67480c350d',\n",
       " '5f5cd2822dc0482ca773b7e31b46b253',\n",
       " '4f2577d89eb04189aef32861bbaadb74',\n",
       " 'cb5efb3353464d91aa0dff3fad211a4f',\n",
       " '893c89c6c05f4267ae9940ed3042d22f',\n",
       " '87d0a7ae307d46e9b04f296cc0c69e5b',\n",
       " '7a4528b963e24a54861bdb3ef14a36f7',\n",
       " 'b986a726a676431a8deb794b236f5c23',\n",
       " '105c4dd48e8e49d8b4337e19f8873682',\n",
       " 'd1b8c2893fd04b3cbd82af5503a0afac',\n",
       " '11136a024f104742a45bd16270d42a3c',\n",
       " '7da514ae0f9d433c93e4fbf484749a5e',\n",
       " 'ea7b00c46ef4468c8834a88d0ed01266',\n",
       " '6ef1cf54cb754567b64038bf6851e175',\n",
       " '884a7ba3b1644895a454287c7b6a601a',\n",
       " 'c0edc367d6a34ec08fa964879e72e1c1',\n",
       " 'b1d0545936004dec894dce348cdd5c42',\n",
       " '92a62e48eb574e34977a167d0bbab363',\n",
       " 'b524bb69c9124f46a907f188d38317d3',\n",
       " '2ffebc92d06247b086d89c4777082e03',\n",
       " '166bf1f6b5414b99b87c9bb17dcaff21',\n",
       " '11ba00861c744565b59811f89b961cf6',\n",
       " 'fe173a0bf66c46beb843000c0b4354b4',\n",
       " '56f3d474bc5a4ba58113b749984a00f2',\n",
       " 'e02cc74423ba4156bd52d6de7272c512',\n",
       " '56bec9e9951642e9b1ebeabbf8fc5b90',\n",
       " 'e461e2cdcd5d4f21996319c5cc29c796',\n",
       " '42ddcd390f3d4593bc122ecff521e506',\n",
       " '18bd9b8aa10f442a9f94c9a13b3a56c4',\n",
       " '9217e5cb2df44c77965a24996364dfc7',\n",
       " '702f17927a674f10bdec31e1ba86749b',\n",
       " '68cb5726fc52411796b4a6088c0aacb4',\n",
       " '63a6febad7f14505b8376ffed25b77c0',\n",
       " 'bb2e769aba9c490d981d56d01267d2f5',\n",
       " 'f3ad025df75546a6bdf87b2365d03e37',\n",
       " '37677f0a4fde498aa9f0ff3845425b87',\n",
       " '1c39c94bb82a4d21af5161d94f03a6e4',\n",
       " '48f973a51457471ba4b24a5bfe44c955',\n",
       " 'b3b328a50f77456d82a669a4e15a942d',\n",
       " '13a87ed17ac64d79ac405259055bc4f5',\n",
       " 'f0b83dff04b04424b07ece04ca5713e4',\n",
       " 'bc2e817e89e643b59b19c47bf9f7ba21',\n",
       " 'a593c8fbacd34e3381fe6c10e7b31900',\n",
       " 'b4469eaef91247e0b13e252d7148905c',\n",
       " 'fc43c5455be3456da5594dbf5df1fa52',\n",
       " 'bdd3d35d8fb440d48d511fb2cf4127d2',\n",
       " 'cf62988a7b15496b8d62152924c3d51e',\n",
       " 'edbf7532257649b0b3d4bcac1d07999e',\n",
       " '7ae73b451c2c45fb80ab0a3111c08f0d',\n",
       " '73abea88ff5b4031857c74754a1a2687',\n",
       " 'f8e7016cec8440879e979f999d3a14c3',\n",
       " '012f1afadd3b489fb5ae4d1873db2cbe',\n",
       " '4500653c0c064a7992fdeddc977b93e4',\n",
       " '473d9e027a61425481fccc87e5fd8269',\n",
       " 'd19b6e5457bf4af9b26c19bd547a2c68',\n",
       " '7c989ae5468c4a508b67b9017047e89e',\n",
       " 'bae0963dc4994e4fb2e93181415600de',\n",
       " 'a808ec2d57114ba98488cedbb1c71cc1',\n",
       " '731df4cdc90c4531b97cccc8c091e88d',\n",
       " '19a905a9ad2c4d4c95b909f0a4f44e2c',\n",
       " '9a58e4a2a66f463aa0aaf7c26fb5191d',\n",
       " '0710fb845575446abc3c19c2c7cc4238',\n",
       " '2ab4bcc678c547b49eeb7a2aff273fe3',\n",
       " '7f1fa75305fd40c8ade98320081bb239',\n",
       " '6d8775622e3649bbb732bb72ae966625',\n",
       " '3b5819de4f7e400aa0c921f781cd6ed7',\n",
       " '8db48ec59eab46a3a9b701315f3540d2',\n",
       " '917d343df35f471fa8c28082de2214f0',\n",
       " '677f9761bfea41acacd6691a0db2a877',\n",
       " '45721e21d8144fe6a943a67d2a9e6e0a',\n",
       " '5cbf179c98b5406c95b6694be5094905',\n",
       " '8be2f70c9c7b48adb44281c9f5d7a1bb',\n",
       " 'ceb0471537f94e09bc2008f7196c5e9f',\n",
       " '0949aba2fed9428893f4cc545f830ca7',\n",
       " 'a18f68e09e294e0e8f148088054e4d07',\n",
       " '92be2db6287e4d94933e8b229a88820a',\n",
       " '9873aceac55b49fd8d97aa730d4fb013',\n",
       " '49c83fb623104b05a77ebea43791470c',\n",
       " '766b2fb60d9b43b2873db474d57e0dd1',\n",
       " 'f98983a1bee24ed3814a949ba98717eb',\n",
       " 'dccef50a638848c39a347a806b0368e9',\n",
       " 'd974c6aee3964920b8efecf9b566aa06',\n",
       " '78b340e59d7444cca777b10db1ea64c4',\n",
       " '117aa26f6a904769a3b1e5d82772b03b',\n",
       " '77c691de4e464ed4a70141ecfba012b4',\n",
       " '9e3034293d1042bbbfa7a471b6498809',\n",
       " 'e8087b6bf17e462eb35b401d2e2dd4eb',\n",
       " 'a412a2993acf46b6ab85ace76984ff07',\n",
       " 'daa6eb0f62f948aeaef95d399f4ecf53',\n",
       " 'bec294eef1974ef8bd9057224f35df20',\n",
       " '2da4c0cc907f40e399e0fc0f4ade9bed',\n",
       " '94ce4d9cd29e45259dd2f796941b8e23',\n",
       " 'b3d5a699450043a3af85869f41d0f19f',\n",
       " '5189a4d59a0b404aa08523c7072dba77',\n",
       " 'c08c48d536864900a18fce26949e2d50',\n",
       " '44e5832e4369436187efbd9ebaafc8d9',\n",
       " '57ef6b7f2d744a01af65933b2d4bec7b',\n",
       " 'd15f8f0d7ead4e78a66652ab212300d5',\n",
       " '0fede6f13f5b43088a52d1566506f1c0',\n",
       " 'e75997ab2f824f8e9e318a1b4e4a2e1f',\n",
       " '2d21cca0eab04eee91a92a1f608a0736',\n",
       " 'ff34fa84a06241e59cf88490fd6b6ed8',\n",
       " 'b41854d5292b4963b197b03030f31ddc',\n",
       " 'd082e237287840da946a2cea754b8dd1',\n",
       " 'be7e33a9349747949e88e8bfbca63f69',\n",
       " 'a80478d5813a4a2c94fc0981a7c2cb44',\n",
       " 'ce8863607edb4f2585ab0468cb298cf2',\n",
       " 'e8b7a168a98146af9ae71de54ecaa9e2',\n",
       " 'acf0880b0cd84fceac7466533a5b0954',\n",
       " '7d8f455fcbdf4436844ec9441af907dd',\n",
       " '6c03b88579a04396939e732111fda3c5',\n",
       " '2029bd79eeae4e35a0335ca94ac04249',\n",
       " 'd31923b19447461e8024259ce1eb85d9',\n",
       " '9b48464c7738455a8ef4e788fcfd2a01',\n",
       " '3790ac31c6134e169ddf33d593a5dc83',\n",
       " 'c97b62d041774a9682d9659141cb48d0',\n",
       " '7552c1298ccb451bb778fa2aa2df67cf',\n",
       " '3e0d1d928aeb4ce6988f0c295272eac0',\n",
       " '935ef91dcd2c4054afc05294ee5ec553',\n",
       " 'e03ae36893614350964630dcfe489114',\n",
       " '11a62fb9c1e14d61926e6595a6b9a4d7',\n",
       " '022d8f4929364f9a883fdc203ec17469',\n",
       " '73d3129385ea4749aef0aabe5608f33d',\n",
       " '1a5a013037b34a8888726907e52dee33',\n",
       " '857124ddaa2e4687a6a5f28b0860f3ba',\n",
       " '4bd965e1dfc249eeaf279c326d4ada60',\n",
       " 'ff18cac93e374365ad34084f4e96cb65',\n",
       " '2c5a8c9384984efe81b0bd43a0c3ec71',\n",
       " '15adc975919b4e6abf91afdc50335e69',\n",
       " '9d8a5eca0cf944e4b02cf46597bcf0dd',\n",
       " '18a71fc1a60d44a5a2ff739bc223f8f0',\n",
       " '1093783f37b54fc9aaa9d596e229b246',\n",
       " 'd89531e36621450ba2304e93e0466f1c',\n",
       " '7ac0326fbcab4f67b70bccb9fa528334',\n",
       " '0dd7848336e44ad99e6d53d8af3b80e3',\n",
       " '74dce2eaf80b4a97a2eff9d2cd87df4c',\n",
       " 'fb36bb1331bd42fba493a06757d747a9',\n",
       " '304df281362e42e0b2510167a19466fb',\n",
       " '37ef5683a4014bac8daa13397b56c9e4',\n",
       " '333d5ae8239e40c5a876d0ba06905ee2',\n",
       " '8127deb3bcae44aea949b7bcaf6a1d3b',\n",
       " '978f3ffe3abc4b3daac213b478611d7b',\n",
       " 'ade4decb0dc64515a6963094ceb254d0',\n",
       " 'f51691bd9e1144aa937753699c57e665',\n",
       " '0527b210e7934a6eb28796bd89e7c3d7',\n",
       " '25778d0ac9084bc582c539b9e4959956',\n",
       " 'de032ce3145d4a4681fe037e45ec7d67',\n",
       " '50e888aaecd441b9a8d65409c849ee6a',\n",
       " 'fe13066f396f48938d293837dbac35c7',\n",
       " '7f463edb5e7e44ff96a5a82f2cb53f96',\n",
       " '4f29f50638924ad6afce3aecf0168b12',\n",
       " '0f8bed00712444488582d0cb0b52bfc8',\n",
       " '03facd44ee4f4bf28ae3b0b2b5425acc',\n",
       " '185a274483b04d48a9edefbb3b4274b1',\n",
       " '74fcb81d7df443eca6568aa0474710c8',\n",
       " '77c3020d3f424684afdb385caf79a18f',\n",
       " '46267466dc60432d98e39b21dec1333e',\n",
       " '338e7800fbcf45099e3a1f5cb10417b6',\n",
       " '2bf935137b46420fae3f8d5862030354',\n",
       " 'bd57ca14098c4a7ead9d3f9bc3874c26',\n",
       " '3c69f335eab84690be1fe38a44ffe6d2',\n",
       " '8aed4df075c84668bb6eeae7d21a4120',\n",
       " 'd6aa95d14bc548ce87afc2d404df240c',\n",
       " '8b01ba06dd654e2c86e90d89a03160a7',\n",
       " '6b211abd0d884e4cbbc7fba65a4efee3',\n",
       " 'acaa9f4870bc4210b3878a405b56afa3',\n",
       " '323ebcfd96f7426d81a8642207f3117b',\n",
       " 'd8ee59b687b14140bbdfb3948d2b82f0',\n",
       " '126d9f0d24854d49a66735602d5e94d9',\n",
       " 'ff4aaa496ef44d15af3bbc5991116ab2',\n",
       " 'a28053a068df429db19bed9f7786006e',\n",
       " 'c4b9b04caa7f4041bc1dd32d46968500',\n",
       " '068d5e942682434fb7afb81b9d8931c4',\n",
       " '7ee3705d149b4a139f999de5945f7097',\n",
       " 'cf70adb2a69c4a8c9947987e2f521234',\n",
       " 'f635b162a175446e92f55484bdbbfa71',\n",
       " 'b7dc5fd38fbb40f48157166a897bf7e4',\n",
       " '045429522f004448bc72f0776c4a1d2e',\n",
       " '67be5199d23b413592d8985c890e877e',\n",
       " 'cd539a9f988b4f97a74a8884385d22ca',\n",
       " '124a1c932ec9426d8d61ad623e0118cf',\n",
       " '3d3b84743eb14376842a753961686a2b',\n",
       " 'b610c8c60f494cd18938aa5ec63f5823',\n",
       " '11481c4285e54cd2b1ad0da67df0f20a',\n",
       " '4947560a166043c18636a35cf03f0a48',\n",
       " '869437270b684854802717b2b5050595',\n",
       " 'e47f040e796644b084fa052057d7cdf5',\n",
       " '109077fff0144cf3a1b1f524b13ff712',\n",
       " '74055e18050c4791b02466069af12705',\n",
       " '873d8c4cb7be425f8169ae6fa56bd99a',\n",
       " '49ad991d57c546df853f22be56aedaf9',\n",
       " '7552675bf6774b71a513484c763f6f1c',\n",
       " 'd4d2c0a50ee24b06bfd54a3700c6edf9',\n",
       " 'de98575714784d11a51a8cc6716b1a04',\n",
       " '67b7f7a7a00c4dfc88ce596de1383788',\n",
       " 'b02cdd8b75b849df867da14e076c41a5',\n",
       " '1162996b28a8492f96e727e34da0cee2',\n",
       " 'eb4e7591fd8442bbb86e04e76d4473b1',\n",
       " 'cb4082e3395f416ca380dd60623b0b06',\n",
       " 'dffc1397ddbf43cd9ee7844f50ea0271',\n",
       " 'd6132ea50dda410f9473dc80c917333b',\n",
       " 'e36b40e49b8c48b1970d104e5d84e381',\n",
       " '1575b248874f4a578f160b46fe885d85',\n",
       " '685fe5209db94eb7ae76f42c7f6acf5a',\n",
       " '0657988247ba4580903642986a7028e5',\n",
       " 'fa3431bc702840c09c81fe60de26d6e2',\n",
       " '1990deb6a05243949d891e632081e548',\n",
       " '1c517eb5add74227afae908eb859f720',\n",
       " 'a42719ccf9024e68af3535d0b557cfc3',\n",
       " '0afb584c418044f4915abddfa8c963a2',\n",
       " 'dd48a5e058de45d58e185f55b2dc288c',\n",
       " 'd84b74a4307e4b9a8874d39d91882b2b',\n",
       " 'f16b21133345492cb4805bb9c986afbb',\n",
       " 'ecc9d0e1aed4483ba47f62ffaa38cb4e',\n",
       " '7052c7ea19824b92afc72709da7e50e7',\n",
       " '718efb4a2394490a94a18f15a93e66b1',\n",
       " '3ad204edced74a6fbbaa99ea57575ead',\n",
       " '7d7864db31694565b4dbbea53a029a4a',\n",
       " 'd511f001e1af47f5857905b5c05e40ec',\n",
       " '6d2d901a37494de1be78813086cf481d',\n",
       " '42905217366d40609baa1eaead490afc',\n",
       " 'a5d634dcebb142ca8cc055269a159dbf',\n",
       " 'def2620f8dc949788409bda6f200d358',\n",
       " 'a1dbe52b72254a1898116b6d40392caf',\n",
       " '71a080b7c166401c9e4572f37053a8ab',\n",
       " '52d0f20c1f554e728d811bd2b084ca95',\n",
       " 'a06f2aac0e5d45bcaf0afb2a2d09b741',\n",
       " 'a7477810f0a649d89cefba1aca9370ed',\n",
       " '887cd5dcb4b44de49b35f72b71d920c3',\n",
       " '3c714cffbd7e4375bb6f2948287e881f',\n",
       " 'b8edf25a8a424229b60895a362adc219',\n",
       " '48d7c662681448ec96c44e546580e985',\n",
       " '33fc96f9af3c488ebdccb5b3012f86d9',\n",
       " 'e2ba48db81a5468e9659f312524405e6',\n",
       " '49b6c02475f24e6ba273804a6eb5d80c',\n",
       " 'b2d95cf006c441e5a1e8184dddbccbf1',\n",
       " '7add0fe765d44ccaa158f5dcb460c06e',\n",
       " 'f781ab13845541eb8a9bcfd1ca5d8081',\n",
       " 'cd1004ae16134412aec2641884604876',\n",
       " '6c3c5777a9604b91ada13221163ae5e0',\n",
       " '8259a9a845ab4de29bd739ba2b78cab2',\n",
       " 'ea1bac2740794897b25bdcd4a503dc4a',\n",
       " 'e7377bc351974f2894d317f4cbb0c6b4',\n",
       " 'c63408fcfa5f421786b286d2d7a285ff',\n",
       " 'c405c5297b5644f892dd910304bc954b',\n",
       " 'a3765ed5eba94d778b72356a8444c52f',\n",
       " '7e0ed267c1b0456bb29b70faf0b2e617',\n",
       " '0637a6e8dac44bafb9fbfe441c4ce76d',\n",
       " '1587a69e083a4983932c0dcf6beb4c93',\n",
       " '62521add7e904dc7a040ff463bac004e',\n",
       " 'c85d785a1614406ebe6d91d9af3ffd85',\n",
       " '38a8194dbce24707a7a9b9ab288f561e',\n",
       " '879feaa35ae84bb7b8a8399c06c889b4',\n",
       " '68c69175a34f4be4a04babf2e717a34a',\n",
       " '6f54e3087cd4406f9fd48b81a204fdde',\n",
       " '037d38495e54408584d3eaf91d0cb6e2']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index documents\n",
    "LOCATION = \":memory:\"\n",
    "COLLECTION_NAME = \"ai-ethics-sdg\"\n",
    "VECTOR_SIZE = 1536\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "qdrant_client = QdrantClient(LOCATION)\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embedding\n",
    ")\n",
    "\n",
    "qdrant_vector_store.add_documents(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43677270-18b9-4b89-a184-70c44dc0e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the RAG retrieval chain\n",
    "from operator import itemgetter\n",
    "prompt = \"\"\"\n",
    "Please answer the question below using the provided context. If the question cannnot be answered\n",
    "using the context, politely state that you can't answer that question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt)\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "retriever = qdrant_vector_store.as_retriever()\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5797dfe-4579-40e2-b71c-04fdca94abb2",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "With the retrieval QA chain ready to test, we generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ed81b-e96c-4cb1-8873-8640f1c1808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.75,\n",
    "    multi_context: 0.20,\n",
    "    reasoning: 0.05\n",
    "}\n",
    "\n",
    "num_qa_pairs = 200 # You can reduce the number of QA pairs to 5 if you're experiencing rate-limiting issues\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(split_documents, num_qa_pairs, distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21828fc6-d4d8-43c9-8fe9-42cab595b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = testset.to_pandas()\n",
    "df.to_csv('golden_eval_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bfc29f3-52b7-4d9f-8b4d-1419aef2abe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can organizations enhance risk management ...</td>\n",
       "      <td>[acquisition, human resources, legal, complian...</td>\n",
       "      <td>Organizations can enhance risk management rega...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'NIST.AI.600-1.pdf', 'file_path': ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is information integrity maintained in the...</td>\n",
       "      <td>[Information Integrity \\nAI Actor Tasks: AI De...</td>\n",
       "      <td>Information integrity in the context of AI dev...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'NIST.AI.600-1.pdf', 'file_path': ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do conventional cybersecurity practices ne...</td>\n",
       "      <td>[11 \\nvalue chain (e.g., data inputs, processi...</td>\n",
       "      <td>Conventional cybersecurity practices may need ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'NIST.AI.600-1.pdf', 'file_path': ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can the verification of fine-tuning ensure...</td>\n",
       "      <td>[MS-2.7-004 \\nIdentify metrics that reﬂect the...</td>\n",
       "      <td>The verification of fine-tuning ensures that s...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'NIST.AI.600-1.pdf', 'file_path': ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are examples of sensitive information tha...</td>\n",
       "      <td>[entities.  \\n7 What is categorized as sensiti...</td>\n",
       "      <td>Examples of sensitive information that may be ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'NIST.AI.600-1.pdf', 'file_path': ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How can organizations enhance risk management ...   \n",
       "1  How is information integrity maintained in the...   \n",
       "2  How do conventional cybersecurity practices ne...   \n",
       "3  How can the verification of fine-tuning ensure...   \n",
       "4  What are examples of sensitive information tha...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [acquisition, human resources, legal, complian...   \n",
       "1  [Information Integrity \\nAI Actor Tasks: AI De...   \n",
       "2  [11 \\nvalue chain (e.g., data inputs, processi...   \n",
       "3  [MS-2.7-004 \\nIdentify metrics that reﬂect the...   \n",
       "4  [entities.  \\n7 What is categorized as sensiti...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  Organizations can enhance risk management rega...         simple   \n",
       "1  Information integrity in the context of AI dev...         simple   \n",
       "2  Conventional cybersecurity practices may need ...         simple   \n",
       "3  The verification of fine-tuning ensures that s...         simple   \n",
       "4  Examples of sensitive information that may be ...         simple   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': 'NIST.AI.600-1.pdf', 'file_path': ...          True  \n",
       "1  [{'source': 'NIST.AI.600-1.pdf', 'file_path': ...          True  \n",
       "2  [{'source': 'NIST.AI.600-1.pdf', 'file_path': ...          True  \n",
       "3  [{'source': 'NIST.AI.600-1.pdf', 'file_path': ...          True  \n",
       "4  [{'source': 'NIST.AI.600-1.pdf', 'file_path': ...          True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7e0f2c-a807-487c-9808-ed9ac8caa236",
   "metadata": {},
   "source": [
    "## Evaluating RAG for text-embedding-3-small Pipeline using RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f735243-1fa8-421c-a617-02631beff270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('golden_eval_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2886a25-a81a-4625-aca4-6d643fd6a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get answers to questions using our rag_chain\n",
    "from tqdm.auto import tqdm\n",
    "test_questions = df[\"question\"].values.tolist()\n",
    "test_groundtruths = df[\"ground_truth\"].values.tolist()\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in tqdm(test_questions):\n",
    "  response = rag_chain.invoke({\"question\": question})\n",
    "  answers.append(response['response'].content)\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38abff0f-6e01-4cf9-97df-1c9dd87f4e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24deeb32212d4417979badf530d9f5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})\n",
    "response_dataset.save_to_disk('te3-responses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98dfcf95-f7f6-4f69-9535-9abdc01d2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66864b2b-407a-42f5-b653-394df0192a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf7bdd35e114e76b9f7035b472ee65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/965 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(response_dataset, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c4ac90d-58d7-423c-b8b5-057daf66beef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.9130, 'answer_relevancy': 0.8136, 'context_recall': 0.9033, 'context_precision': 0.8869, 'answer_correctness': 0.6251}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7ff9c-db0c-40a6-82fc-5c0b93a770e7",
   "metadata": {},
   "source": [
    "Looks like `text-embedding-3-small` model performs quite well out of the box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8741fc9-cd72-47c9-85c4-60f72406a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results\n",
    "import pickle\n",
    "with open('te3-eval-results', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f937e94-d505-4b87-8a23-d6cb408da4f1",
   "metadata": {},
   "source": [
    "## Evaluating RAG for text-embedding-3-large and SemanticChunker Pipeline using RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d20f1f10-5e4c-4d01-bb9c-2a46f3d5a672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 0.0.3 requires langchain-core<0.3,>=0.1.52, but you have langchain-core 0.3.5 which is incompatible.\n",
      "langchain-openai 0.1.25 requires langchain-core<0.3.0,>=0.2.40, but you have langchain-core 0.3.5 which is incompatible.\n",
      "ragas 0.1.20 requires langchain-core<0.3, but you have langchain-core 0.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc106f9f-6b79-4a09-aabe-23644a29ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings(model='text-embedding-3-large'))\n",
    "split_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26f3aa07-906b-494c-8969-30f91e37f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the semantically chunked docs because they are hard to compute\n",
    "with open('semantic_chunked_docs', 'wb') as f:\n",
    "    pickle.dump(split_documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8e889c7-ce18-4ebb-9b7c-563c21aceb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Documents\n",
    "# Index documents\n",
    "LOCATION = \":memory:\"\n",
    "COLLECTION_NAME = \"ai-ethics-sdg-te3-large-semantic\"\n",
    "VECTOR_SIZE = 3072\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "\n",
    "qdrant_client = QdrantClient(LOCATION)\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embedding\n",
    ")\n",
    "\n",
    "_ = qdrant_vector_store.add_documents(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "203302c3-63a1-4c5f-933c-10f14fc918f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the RAG retrieval chain\n",
    "from operator import itemgetter\n",
    "prompt = \"\"\"\n",
    "Please answer the question below using the provided context. If the question cannnot be answered\n",
    "using the context, politely state that you can't answer that question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt)\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "retriever = qdrant_vector_store.as_retriever()\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "afafcf03-4539-4f73-ae71-ee02d4679899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfaef5298b74e5f889cda33744b8da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get answers to questions using our rag_chain\n",
    "from tqdm.auto import tqdm\n",
    "test_questions = df[\"question\"].values.tolist()\n",
    "test_groundtruths = df[\"ground_truth\"].values.tolist()\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in tqdm(test_questions):\n",
    "  response = rag_chain.invoke({\"question\": question})\n",
    "  answers.append(response['response'].content)\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d617d03a-6750-4958-8577-482a77c8999b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3321ac56b31840c0b3e22f0147378660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})\n",
    "response_dataset.save_to_disk('te3-large-responses-semantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0af917e0-39ae-432b-a78b-fcc30efbfc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90963a2970b94ccc831f895f760b114a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/965 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse output. Returning None.\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "]\n",
    "\n",
    "results = evaluate(response_dataset, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9164f459-e191-48da-be2c-561f366db213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.9020, 'answer_relevancy': 0.8151, 'context_recall': 0.8581, 'context_precision': 0.9013, 'answer_correctness': 0.6180}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02b09089-a257-4109-b180-0feb9fc9285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results\n",
    "import pickle\n",
    "with open('te3-large-semantic-eval-results', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e006dced-c289-462b-b1cd-de1e980b4598",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('te3-eval-results', 'rb') as f:\n",
    "    results_te3_small = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "25d2cfd1-f367-417b-a64d-e6ed84dc1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('te3-large-semantic-eval-results', 'rb') as f:\n",
    "    results_te3_large_semantic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7835abcf-8007-4273-a685-baebaf6de682",
   "metadata": {},
   "source": [
    "# Comparing the two Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68779b22-ccdd-46e9-8781-b3ab19a973c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>TE3-Small</th>\n",
       "      <th>TE3-Large-Semantic</th>\n",
       "      <th>TE3-Small -&gt; TE3-Large-Semantic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.912950</td>\n",
       "      <td>0.902031</td>\n",
       "      <td>-0.010920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.813614</td>\n",
       "      <td>0.815119</td>\n",
       "      <td>0.001505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.903331</td>\n",
       "      <td>0.858105</td>\n",
       "      <td>-0.045226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.886874</td>\n",
       "      <td>0.901267</td>\n",
       "      <td>0.014393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer_correctness</td>\n",
       "      <td>0.625057</td>\n",
       "      <td>0.618008</td>\n",
       "      <td>-0.007049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Metric  TE3-Small  TE3-Large-Semantic  \\\n",
       "0        faithfulness   0.912950            0.902031   \n",
       "1    answer_relevancy   0.813614            0.815119   \n",
       "2      context_recall   0.903331            0.858105   \n",
       "3   context_precision   0.886874            0.901267   \n",
       "4  answer_correctness   0.625057            0.618008   \n",
       "\n",
       "   TE3-Small -> TE3-Large-Semantic  \n",
       "0                        -0.010920  \n",
       "1                         0.001505  \n",
       "2                        -0.045226  \n",
       "3                         0.014393  \n",
       "4                        -0.007049  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_te3_small = pd.DataFrame(list(results_te3_small.items()), columns=['Metric', 'TE3-Small'])\n",
    "df_te3_large_semantic = pd.DataFrame(list(results_te3_large_semantic.items()), columns=['Metric', 'TE3-Large-Semantic'])\n",
    "df_merged = pd.merge(df_te3_small, df_te3_large_semantic, on='Metric')\n",
    "df_merged['TE3-Small -> TE3-Large-Semantic'] = df_merged['TE3-Large-Semantic'] - df_merged['TE3-Small']\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a138324-0695-46c5-b294-9918f1a45523",
   "metadata": {},
   "source": [
    "The above chart shows that while there are minor changes in some of the metrics, **there is no significant difference in these two chunking strategies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09595689-9448-450c-a99a-483938de75d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
