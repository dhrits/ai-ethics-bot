{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df90de8-ed96-4ec9-93c9-5f54ab0e3e38",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook aims to build a searchable vecstore of the documents [Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People](https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf) and [National Institute of Standards and Technology (NIST) Artificial Intelligent Risk Management Framework](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf) in order to help allay the concerns of people who are anxious about the state of AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4dedb-83b6-4c65-a984-bad0c07ffd2b",
   "metadata": {},
   "source": [
    "We'll start with a set of imports to get ready for indexing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aaa022a-c844-40ba-8f76-046ce2f99a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredHTMLLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74884c02-33cf-4b7d-8ab2-4ac4420e0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv; _ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cc0593-8df1-4735-a56c-9ca293c6d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyMuPDFLoader('Blueprint-for-an-AI-Bill-of-Rights.pdf')\n",
    "docs_bill_of_rights = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3160e9-de23-48ab-b855-f32cf0144f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyMuPDFLoader('NIST.AI.600-1.pdf')\n",
    "docs_nist = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51c4efb9-b603-47c5-a6d4-e815a125c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = docs_bill_of_rights + docs_nist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1147646b-e5b7-45c5-9f99-e81c6b062d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28893d5-611b-4712-86e2-76cb5487db44",
   "metadata": {},
   "source": [
    "## Text splitting\n",
    "Next we take these documents and split them up into chunks for easy retrieval from a vectorstore. Without knowing much about these documents, a `RecursiveCharacterTextSplitter` seems like the most obvious choice. Even the Langchain website recommends this strategy if the data is mostly unstructured (which these PDF documents are) and there's no additional structure we know about it. We'll make use of `text-embedding-3-small` as the default choice of openai embeddings for maximal performance. This is from the family of the highest performance embedding models from OpenAI. Using the `small` embeddings trades off performance and cost.\n",
    "\n",
    "Since we're using `text-embedding-3-small` model, we'll also define a length function that accounts for tokens from use of this model while splitting up the text into chunks. \n",
    "\n",
    "For the future, a chunking strategy we could test out the experimental `SemanticChunker` which further combines sentences if they are semantically similar. But this is more of a risky choice so we'll stick with the default for now and might make use of MDD to determine if `SemanticChunker` is better later. We cam additionally also try out `text-embedding-3-large` model with this strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad93c24a-c5d8-49ab-96c1-4254a5c57f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiktoken_len(text, model='text-embedding-3-small'):\n",
    "    embedding = tiktoken.encoding_for_model(model)\n",
    "    query = embedding.encode(text)\n",
    "    return len(query)\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    length_function=tiktoken_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f53fadf-a082-4e4e-a23a-e8ee74692e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3378c36a-29ff-4b9b-9d33-1595fd52fa73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82fa34-5c2e-4586-b65c-e817e0791edf",
   "metadata": {},
   "source": [
    "## Build Vectorstore from Embeddings\n",
    "Next we take these split documents and build out a vectorstore using `Qdrant`, a fairly high performant and flexible vectorstore. We'll continue to use `text-embedding-3-small` as the embedding function to store documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c44ac394-797d-4ca8-a2af-44708b679302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv; load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "551eca15-9045-4868-95f6-5d2276e2d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2604cc-5b27-4ace-a921-0c0422df4bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\n",
    "    url=os.environ.get('QDRANT_DB'),\n",
    "    api_key=os.environ.get('QDRANT_API_KEY'),\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"ai_ethics_te3_small\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d9fbda8-bc24-41ec-b2ba-52d6245fe833",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"ai_ethics_te3_small\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55c83411-47ad-4854-ac02-f89f37baacbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents(store, documents):\n",
    "    for i in range(0, len(documents), 10):\n",
    "        batch = documents[i:i+10]\n",
    "        vector_store.add_documents(\n",
    "            documents=batch,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7112a4-80c7-4168-9065-189c934374d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_documents(vector_store, split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ddef8b-a655-44e2-a57c-7f7ad590d173",
   "metadata": {},
   "source": [
    "## Also add a store for text-embeddings-3-large\n",
    "Based on evaluation done in the notebook `Test Data and RAGAS Evaluation.ipynb`, it appears that a split and indexing strategy based on `text-embedding-3-large` model performs slightly better on some key metrics compared to `text-embedding-3-small` model. As such, we'll also create a vectorstore based on this embedding.\n",
    "\n",
    "It didn't appear that SemanticChunking made much of a difference, so we ignore this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "08ec6cfa-7448-4ac6-b2c8-6b72e77b31f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    length_function=partial(tiktoken_len, model='text-embedding-3-large')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bded97b1-8962-4f32-bb4c-d51f1b2ed888",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb4f17b6-26bf-4f15-91c7-0ea8d4ba7345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"ai_ethics_te3_large\",\n",
    "    vectors_config=VectorParams(size=3072, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b0f4c92-095f-408c-91f5-288f5659dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"ai_ethics_te3_large\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd35bc15-9e58-4f08-bd29-4eb57c14dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_documents(vector_store, split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05304b32-dfc3-4bfe-b688-39dd7a9dbe81",
   "metadata": {},
   "source": [
    "# Updating Vectorstore with Policy Updates\n",
    "\n",
    "There have been documents which have since updated the state of the Government's political stance on AI Systems. This part of the notebook aims to udpate the vectorstore with the executive order on [Safe, Secure and Trustworthy AI](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/) as well as the [270 day update](https://www.whitehouse.gov/briefing-room/statements-releases/2024/07/26/fact-sheet-biden-harris-administration-announces-new-ai-actions-and-receives-additional-major-voluntary-commitment-on-ai/) on the same Executive Order. \n",
    "\n",
    "Any new policy documents can be similarly ingested into our vectorstore(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a92b4b-1a2f-4b2b-b24c-7f8b1a91edeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b541ad8-5022-4ae7-b47e-4156dcb7961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fad1bb-2d54-49b4-bb46-6e3dd0976dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv; load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60251edf-5026-409c-883e-176b0e13d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eo_link = 'https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence'\n",
    "eo_update_link = 'https://www.whitehouse.gov/briefing-room/statements-releases/2024/07/26/fact-sheet-biden-harris-administration-announces-new-ai-actions-and-receives-additional-major-voluntary-commitment-on-ai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca58442-0bac-4347-ad8f-db3f003ef0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def download_html(link, path=None):\n",
    "    if not path:\n",
    "        path = link.split('/')[-1]\n",
    "        if not path.endswith('.html'):\n",
    "            path += '.html'\n",
    "    \n",
    "    with open(path, 'wb') as f:\n",
    "        iter = requests.get(link, stream=True)\n",
    "        for r in iter.iter_content(chunk_size=1024):\n",
    "            f.write(r)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b3418-589f-45e3-8301-b017d4d549a9",
   "metadata": {},
   "source": [
    "## Update Vectorstore of `text-embedding-3-small` embeddings\n",
    "First update the vectorstore of `text-embedding-3-small` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b9f8fd6-f24c-4593-857b-898e6d195fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eo = download_html(eo_link)\n",
    "eo_documents = UnstructuredHTMLLoader(eo).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "506f0764-bd8b-4f70-ac4c-02e149a27e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "eo_update = download_html(eo_update_link)\n",
    "eo_update_documents = UnstructuredHTMLLoader(eo_update).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6bb99ff-aee8-4376-866e-e19d16bbb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eo_documents = eo_documents + eo_update_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "547f1724-5cb1-4b81-8259-d2979ec74ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    length_function=partial(tiktoken_len, model='text-embedding-3-small')\n",
    ")\n",
    "eo_split_documents = text_splitter.split_documents(all_eo_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39fad67d-1834-413d-99e0-4579e7b2d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_s/6j2r22wj52g7wsz019z7jgsm0000gn/T/ipykernel_66079/4088305373.py:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings(model='text-embedding-3-small')\n"
     ]
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "client = QdrantClient(\n",
    "    url=os.environ.get('QDRANT_DB'),\n",
    "    api_key=os.environ.get('QDRANT_API_KEY'),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"ai_ethics_te3_small\",\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7d7ae4c-058f-4fa0-a235-793c3de4fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_documents(vector_store, eo_split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d4d0f4-a112-4c2a-9b3a-f8ad76416c4a",
   "metadata": {},
   "source": [
    "## Update Vectorstore of `text-embedding-3-large` embeddings\n",
    "Also update the vectorstore indexed using `text-embedding-3-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61bc175d-e912-4f79-9f82-e6d30db5e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    length_function=partial(tiktoken_len, model='text-embedding-3-large')\n",
    ")\n",
    "eo_split_documents = text_splitter.split_documents(all_eo_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af16ec86-1031-4a03-a564-f552af0026d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "client = QdrantClient(\n",
    "    url=os.environ.get('QDRANT_DB'),\n",
    "    api_key=os.environ.get('QDRANT_API_KEY'),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"ai_ethics_te3_large\",\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50d29f08-c769-4bb7-83c1-6420764214fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_documents(vector_store, eo_split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3240e-1946-45b9-81e4-dce221b5f1d8",
   "metadata": {},
   "source": [
    "# Add a vectorstore for `nomic-embed-text-v1` finetuned model\n",
    "The notebook `Fine_Tuning_nomic_embed_text_v1_on_AI_Ethics_Docs.ipynb` further finetunes a [nomic-ai/nomic-embed-text-v1(https://huggingface.co/nomic-ai/nomic-embed-text-v1) model. This model outperforms default models on Answer correctness. Thus we finally create a vectorstore indexed with our finetuned embedding model and with all the documents above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "834d4ddb-b9ac-4f80-9696-74493c9a57ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyMuPDFLoader('Blueprint-for-an-AI-Bill-of-Rights.pdf')\n",
    "docs_bill_of_rights = pdf_loader.load()\n",
    "pdf_loader = PyMuPDFLoader('NIST.AI.600-1.pdf')\n",
    "docs_nist = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e851cd57-5a88-4b74-bdcd-0f9246cd042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eo_documents = UnstructuredHTMLLoader('executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence.html').load()\n",
    "eo_update_documents = UnstructuredHTMLLoader('fact-sheet-biden-harris-administration-announces-new-ai-actions-and-receives-additional-major-voluntary-commitment-on-ai.html').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a05bfc9-aec3-4685-ab84-90301757ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = docs_bill_of_rights + docs_nist + eo_documents + eo_update_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ae7c77-2d1c-400e-ba8d-a47b0c2829df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deman/Dev/Maven/MavenAIBootcamp/ai-ethics-bot/venv-jupyter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/_s/6j2r22wj52g7wsz019z7jgsm0000gn/T/ipykernel_7729/676822847.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"deman539/nomic-embed-text-v1\", model_kwargs={'trust_remote_code': True})\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"deman539/nomic-embed-text-v1\", model_kwargs={'trust_remote_code': True})\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deman539/nomic-embed-text-v1\")\n",
    "\n",
    "def nomic_len_function(text):\n",
    "  inputs = tokenizer(text)\n",
    "  return len(inputs.input_ids)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    length_function=nomic_len_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fbe17f4-d7e6-4f1b-b368-4d38df7016d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a264ce2-ecf2-4e5e-abcc-228e59c5ca3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = QdrantClient(\n",
    "    url=os.environ.get('QDRANT_DB'),\n",
    "    api_key=os.environ.get('QDRANT_API_KEY'),\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"ai_ethics_nomicv1_finetuned\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa9af84b-d71e-4ce5-ae6a-0bd6dc1d4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"ai_ethics_nomicv1_finetuned\",\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "835e81e3-89f9-4586-a5d3-b615539ce43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_documents(vector_store, split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba01e3-c558-47be-b586-036da6824f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
